{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise       0\n",
      "hypothesis    0\n",
      "label         0\n",
      "dtype: int64\n",
      "premise       0\n",
      "hypothesis    0\n",
      "label         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\38673\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\38673\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['premise', 'hypothesis', 'label'], dtype='object')\n",
      "Index(['premise', 'hypothesis', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')\n",
    "df_val = pd.read_csv('dev.csv')\n",
    "df = df.dropna()\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "df = df[df['premise']!=df['hypothesis']]\n",
    "df = df.drop_duplicates(subset=['premise', 'hypothesis'])\n",
    "df.to_csv('cleaned_train.csv',index = False)\n",
    "df_val = df_val.dropna()\n",
    "missing_values_val = df_val.isnull().sum()\n",
    "print(missing_values)\n",
    "df_val = df_val[df_val['premise']!=df_val['hypothesis']]\n",
    "df_val = df_val.drop_duplicates(subset=['premise', 'hypothesis'])\n",
    "df_val = df_val.dropna()\n",
    "df_val.to_csv('cleaned_val.csv',index = False)\n",
    "# preprocessing\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import download\n",
    "\n",
    "download('stopwords')\n",
    "download('punkt')\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"对文本进行预处理\"\"\"\n",
    "\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text).lower()\n",
    "\n",
    "\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "\n",
    "df = pd.read_csv('cleaned_train.csv')\n",
    "df_val = pd.read_csv('cleaned_val.csv')\n",
    "\n",
    "print(df.columns)\n",
    "print(df_val.columns)\n",
    "\n",
    "\n",
    "df['premise'] = df['premise'].apply(preprocess_text)\n",
    "df['hypothesis'] = df['hypothesis'].apply(preprocess_text)\n",
    "df_val['premise'] = df_val['premise'].apply(preprocess_text)\n",
    "df_val['hypothesis'] = df_val['hypothesis'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "label_data_train = df[\"label\"].tolist()\n",
    "premise_data_train = df[\"premise\"].tolist()\n",
    "hypothesis_data_train = df[\"hypothesis\"].tolist()\n",
    "\n",
    "label_data_val = df_val[\"label\"].tolist()\n",
    "premise_data_val = df_val[\"premise\"].tolist()\n",
    "hypothesis_data_val = df_val[\"hypothesis\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\38673\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\38673\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def find_antonyms(word):\n",
    "    antonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                antonyms.append(lemma.antonyms()[0].name())\n",
    "    return set(antonyms)\n",
    "\n",
    "def antonym_features(sentence1, sentence2):\n",
    "    words1 = set(word_tokenize(sentence1))\n",
    "    words2 = set(word_tokenize(sentence2))\n",
    "    antonyms1 = {ant for word in words1 for ant in find_antonyms(word)}\n",
    "    antonyms2 = {ant for word in words2 for ant in find_antonyms(word)}\n",
    "    return len(antonyms1.intersection(antonyms2))\n",
    "\n",
    "\n",
    "ft_model = fasttext.load_model('cc.en.300.bin')\n",
    "\n",
    "def pair_sentence_vectors(sentence1, sentence2):\n",
    "    sentence_vector1 = ft_model.get_sentence_vector(sentence1)\n",
    "    sentence_vector2 = ft_model.get_sentence_vector(sentence2)\n",
    "\n",
    "\n",
    "    cos_similarity = cosine_similarity([sentence_vector1], [sentence_vector2])[0, 0]\n",
    "    abs_diff = np.abs(sentence_vector1 - sentence_vector2)\n",
    "    elem_mul = sentence_vector1 * sentence_vector2\n",
    "    len1 = len(word_tokenize(sentence1))\n",
    "    len2 = len(word_tokenize(sentence2))\n",
    "\n",
    "\n",
    "    antonym_match_count = antonym_features(sentence1, sentence2)\n",
    "\n",
    "\n",
    "    combined_vector = np.concatenate((sentence_vector1, sentence_vector2, abs_diff, elem_mul, [cos_similarity, len1, len2, antonym_match_count]))\n",
    "    \n",
    "    return combined_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.58      0.60      3233\n",
      "           1       0.63      0.67      0.65      3452\n",
      "\n",
      "    accuracy                           0.63      6685\n",
      "   macro avg       0.63      0.63      0.63      6685\n",
      "weighted avg       0.63      0.63      0.63      6685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 假设 ft_model 已经加载且可用\n",
    "def generate_features(premises, hypotheses):\n",
    "    features = [pair_sentence_vectors(p, h) for p, h in zip(premises, hypotheses)]\n",
    "    return np.array(features)\n",
    "\n",
    "# 生成训练和验证数据的特征向量\n",
    "X_train = generate_features(premise_data_train, hypothesis_data_train)\n",
    "X_val = generate_features(premise_data_val, hypothesis_data_val)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 初始化SVM分类器\n",
    "# svm_classifier = SVC(C=0.1,kernel='sigmoid')  # 你可以选择使用不同的核函数，如 'rbf'\n",
    "svm_classifier = LinearSVC(C=1,loss = \"squared_hinge\",dual = False)\n",
    "svm_classifier.fit(X_train, label_data_train)  # 训练模型\n",
    "\n",
    "# 使用验证集评估模型\n",
    "predictions = svm_classifier.predict(X_val)\n",
    "\n",
    "# 输出分类报告\n",
    "print(classification_report(label_data_val, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('test.csv')\n",
    "premise_data_test = df_2[\"premise\"].tolist()\n",
    "hypothesis_data_test = df_2[\"hypothesis\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame()\n",
    "X_test = generate_features(premise_data_test, hypothesis_data_test)\n",
    "\n",
    "predicted_labels = svm_classifier.predict(X_test)\n",
    "\n",
    "df_new['prediction'] = predicted_labels\n",
    "\n",
    "df_new.to_csv('Group_36_A.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in h:\\miniconda3.8\\lib\\site-packages (1.3.2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_classifier.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install joblib\n",
    "from joblib import dump\n",
    "\n",
    "dump(svm_classifier, 'svm_classifier.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demoes Goes Here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# load the model \n",
    "from joblib import load\n",
    "svm_classifier = load('svm_classifier.joblib')\n",
    "sentence_1 = \"thanks\"\n",
    "sentence_2 = \"thank you\"\n",
    "\n",
    "#load fast text \n",
    "ft_model = fasttext.load_model('cc.en.300.bin')\n",
    "\n",
    "def pair_sentence_vectors(sentence1, sentence2):\n",
    "    sentence_vector1 = ft_model.get_sentence_vector(sentence1)\n",
    "    sentence_vector2 = ft_model.get_sentence_vector(sentence2)\n",
    "    cos_similarity = cosine_similarity([sentence_vector1], [sentence_vector2])[0, 0]\n",
    "    abs_diff = np.abs(sentence_vector1 - sentence_vector2)\n",
    "    elem_mul = sentence_vector1 * sentence_vector2\n",
    "    len1 = len(word_tokenize(sentence1))\n",
    "    len2 = len(word_tokenize(sentence2))\n",
    "\n",
    "\n",
    "    antonym_match_count = antonym_features(sentence1, sentence2)\n",
    "\n",
    "    combined_vector = np.concatenate((\n",
    "        sentence_vector1, \n",
    "        sentence_vector2, \n",
    "        abs_diff, \n",
    "        elem_mul, \n",
    "        [cos_similarity, len1, len2, antonym_match_count]\n",
    "    ))\n",
    "\n",
    "    return combined_vector\n",
    "    \n",
    "def generate_feature(premise, hypothese):\n",
    "    features = pair_sentence_vectors(premise,hypothese) \n",
    "    return np.array(features)\n",
    "\n",
    "#predict \n",
    "Input_vectors = generate_feature(sentence_1,sentence_2)\n",
    "prediction = svm_classifier.predict(Input_vectors.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
