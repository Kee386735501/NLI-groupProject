{"cells":[{"cell_type":"markdown","metadata":{"id":"Y7NMvySuFcuw"},"source":["LSTM + Pytorch For User Stories Classification"]},{"cell_type":"markdown","metadata":{"id":"tTSOAmz1Fcuy"},"source":["1.1 Preprocess the data"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2124,"status":"ok","timestamp":1713967981896,"user":{"displayName":"Kang Fan","userId":"02418101774769161322"},"user_tz":-60},"id":"hGsMOFHtJbwR","outputId":"1a968877-b0a2-4428-f0e6-1d56807fed50"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"GH0Y3EKCbBy4"},"source":["### The first part is training part"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":292,"status":"ok","timestamp":1713967982186,"user":{"displayName":"Kang Fan","userId":"02418101774769161322"},"user_tz":-60},"id":"nvO8KSocFcuz"},"outputs":[],"source":["# load dataset\n","import pandas as pd\n","df = pd.read_csv('train.csv')\n","premise_data = df['premise'].tolist()\n","hypothesis_data = df['hypothesis'].tolist()\n","label_data = df['label'].tolist()"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4874,"status":"ok","timestamp":1713967987058,"user":{"displayName":"Kang Fan","userId":"02418101774769161322"},"user_tz":-60},"id":"aLYTFbwwIQWA","outputId":"78840487-a4f1-47b3-98fa-38f0f2f4ff06"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found GPU at: /device:GPU:0\n"]}],"source":["import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1644,"status":"ok","timestamp":1713967988700,"user":{"displayName":"Kang Fan","userId":"02418101774769161322"},"user_tz":-60},"id":"ReX5knbWFcu1"},"outputs":[],"source":["import re\n","\n","# data clean\n","paired_data = list(zip(premise_data, hypothesis_data, label_data))\n","\n","duplicates = set()\n","unique_paired_data = []\n","for pair in paired_data:\n","    if (pair in duplicates) or (pair[0] == pair[1]) or len(pair[0]) == 0 or len(pair[1]) == 0:\n","        continue\n","    else:\n","        duplicates.add(pair)\n","        unique_paired_data.append(pair)\n","\n","premise_data, hypothesis_data, label_data = zip(*unique_paired_data)\n","\n","premise_data_clean_garbled = [re.sub(r'[^a-zA-Z0-9\\s]', '', text) for text in premise_data]\n","hypothesis_data_clean_garbled = [re.sub(r'[^a-zA-Z0-9\\s]', '', text) for text in hypothesis_data]\n","\n","cleaned_premise_data = [' '.join(re.sub(r'\\b\\w*www\\w*\\b', '', text).split()) for text in premise_data_clean_garbled]\n","cleaned_hypothesis_data = [' '.join(re.sub(r'\\b\\w*www\\w*\\b', '', text).split()) for text in hypothesis_data_clean_garbled]"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24941,"status":"ok","timestamp":1713968013639,"user":{"displayName":"Kang Fan","userId":"02418101774769161322"},"user_tz":-60},"id":"xz6kpCprFcu1","outputId":"716afc3b-d6a5-446d-d936-324bb1f3005c"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","# preprocessing\n","preprocessed_premise_data = [word_tokenize(text) for text in cleaned_premise_data]\n","filtered_premise_data = [[word.lower() for word in premise if word.lower() not in stop_words] for premise in preprocessed_premise_data]\n","\n","preprocessed_hypothesis_data = [word_tokenize(text) for text in cleaned_hypothesis_data]\n","filtered_hypothesis_data = [[word.lower() for word in hypothesis if word.lower() not in stop_words] for hypothesis in preprocessed_hypothesis_data]\n","\n","lemmatized_premise_data = [[lemmatizer.lemmatize(word) for word in premise] for premise in filtered_premise_data]\n","lemmatized_hypothesis_data = [[lemmatizer.lemmatize(word) for word in hypothesis] for hypothesis in filtered_hypothesis_data]"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1713968013640,"user":{"displayName":"Kang Fan","userId":"02418101774769161322"},"user_tz":-60},"id":"7tgPmyuZFcu1"},"outputs":[],"source":["all_data = filtered_premise_data + filtered_hypothesis_data\n","# all_data = lemmatized_premise_data + lemmatized_hypothesis_data\n","\n","vocab_set = set(word for text in all_data for word in text)\n","\n","vocab = list(vocab_set)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":5016,"status":"ok","timestamp":1713968018647,"user":{"displayName":"Kang Fan","userId":"02418101774769161322"},"user_tz":-60},"id":"0zd-AklDFcu2"},"outputs":[],"source":["from gensim.models import Word2Vec\n","from tensorflow.keras.layers import Embedding\n","import numpy as np\n","\n","# load Word2Vec\n","model = Word2Vec(filtered_premise_data+filtered_hypothesis_data, vector_size=200, window=5, min_count=5)\n","# model = Word2Vec(lemmatized_premise_data+lemmatized_hypothesis_data, vector_size=64, window=5, min_count=5)\n","\n","from gensim.models import KeyedVectors\n","\n","word_vectors = model.wv\n","\n","vocab_size = len(vocab)\n","vector_size = word_vectors.vector_size\n","\n","embedding_matrix = np.zeros((vocab_size, vector_size))\n","\n","for i, word in enumerate(vocab):\n","    if word in word_vectors:\n","        embedding_matrix[i] = word_vectors[word]"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":723,"status":"ok","timestamp":1713968019360,"user":{"displayName":"Kang Fan","userId":"02418101774769161322"},"user_tz":-60},"id":"14yp1tB1Fcu4"},"outputs":[],"source":["# load Word2Vec word vectors\n","premise_vectors = []\n","for sentence in filtered_premise_data:\n","# for sentence in lemmatized_premise_data:\n","    sentence_vectors = []\n","    for word in sentence:\n","        try:\n","            sentence_vectors.append(word_vectors.key_to_index[word])\n","        except KeyError:\n","            sentence_vectors.append(vector_size)\n","    premise_vectors.append(sentence_vectors)\n","\n","hypothesis_vectors = []\n","for sentence in filtered_hypothesis_data:\n","# for sentence in lemmatized_hypothesis_data:\n","    sentence_vectors = []\n","    for word in sentence:\n","        try:\n","            sentence_vectors.append(word_vectors.key_to_index[word])\n","        except KeyError:\n","            sentence_vectors.append(vector_size)\n","    hypothesis_vectors.append(sentence_vectors)\n","\n","label_data_list = list(label_data)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1713968019361,"user":{"displayName":"Kang Fan","userId":"02418101774769161322"},"user_tz":-60},"id":"16o7pPaKyxuL"},"outputs":[],"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","# divide data into train set and validation set\n","p_train, p_val, h_train, h_val, label_train, label_val = train_test_split(\n","    premise_vectors, hypothesis_vectors, label_data_list, test_size=0.2, random_state=42)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1713968019361,"user":{"displayName":"Kang Fan","userId":"02418101774769161322"},"user_tz":-60},"id":"VKv3KN2FFcu5"},"outputs":[],"source":["# find max length for padding\n","maxlen1 = int(np.max([len(text) for text in premise_vectors]))\n","maxlen2 = int(np.max([len(text) for text in hypothesis_vectors]))\n","maxlen1 = np.max([maxlen1, maxlen2])\n","maxlen2 = maxlen1\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":314,"status":"ok","timestamp":1713968019672,"user":{"displayName":"Kang Fan","userId":"02418101774769161322"},"user_tz":-60},"id":"axrOLWa5Fcu5"},"outputs":[],"source":["# padding\n","irregular_array = np.array(p_val, dtype=object)\n","padded_p_val = pad_sequences(irregular_array, padding='post', dtype='float32', maxlen=maxlen1)\n","irregular_array = np.array(h_val, dtype=object)\n","padded_h_val = pad_sequences(irregular_array, padding='post', dtype='float32', maxlen=maxlen2)\n","padded_label_val = np.array(label_val)\n","\n","\n","irregular_array = np.array(p_train, dtype=object)\n","padded_p_train = pad_sequences(irregular_array, padding='post', dtype='float32', maxlen=maxlen1)\n","irregular_array = np.array(h_train, dtype=object)\n","padded_h_train = pad_sequences(irregular_array, padding='post', dtype='float32', maxlen=maxlen2)\n","padded_label_train = np.array(label_train)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2748,"status":"ok","timestamp":1713968022417,"user":{"displayName":"Kang Fan","userId":"02418101774769161322"},"user_tz":-60},"id":"jzxA0tdrFcu5","outputId":"897d3976-a9df-4697-d171-bce7f9d0605e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_1 (InputLayer)        [(None, 119)]                0         []                            \n","                                                                                                  \n"," input_2 (InputLayer)        [(None, 119)]                0         []                            \n","                                                                                                  \n"," embedding (Embedding)       (None, 119, 200)             7692400   ['input_1[0][0]',             \n","                                                                     'input_2[0][0]']             \n","                                                                                                  \n"," dropout (Dropout)           (None, 119, 200)             0         ['embedding[0][0]']           \n","                                                                                                  \n"," dropout_1 (Dropout)         (None, 119, 200)             0         ['embedding[1][0]']           \n","                                                                                                  \n"," bidirectional (Bidirection  (None, 119, 128)             135680    ['dropout[0][0]',             \n"," al)                                                                 'dropout_1[0][0]']           \n","                                                                                                  \n"," dropout_2 (Dropout)         (None, 119, 128)             0         ['bidirectional[0][0]']       \n","                                                                                                  \n"," dropout_3 (Dropout)         (None, 119, 128)             0         ['bidirectional[1][0]']       \n","                                                                                                  \n"," dot (Dot)                   (None, 119, 119)             0         ['dropout_2[0][0]',           \n","                                                                     'dropout_3[0][0]']           \n","                                                                                                  \n"," lambda (Lambda)             (None, 119, 119)             0         ['dot[0][0]']                 \n","                                                                                                  \n"," lambda_1 (Lambda)           (None, 119, 119)             0         ['dot[0][0]']                 \n","                                                                                                  \n"," dot_1 (Dot)                 (None, 119, 128)             0         ['lambda[0][0]',              \n","                                                                     'dropout_3[0][0]']           \n","                                                                                                  \n"," dot_2 (Dot)                 (None, 119, 128)             0         ['lambda_1[0][0]',            \n","                                                                     'dropout_2[0][0]']           \n","                                                                                                  \n"," subtract (Subtract)         (None, 119, 128)             0         ['dropout_2[0][0]',           \n","                                                                     'dot_1[0][0]']               \n","                                                                                                  \n"," subtract_1 (Subtract)       (None, 119, 128)             0         ['dropout_3[0][0]',           \n","                                                                     'dot_2[0][0]']               \n","                                                                                                  \n"," multiply (Multiply)         (None, 119, 128)             0         ['dropout_2[0][0]',           \n","                                                                     'dot_1[0][0]']               \n","                                                                                                  \n"," lambda_2 (Lambda)           (None, 119, 128)             0         ['subtract[0][0]']            \n","                                                                                                  \n"," multiply_1 (Multiply)       (None, 119, 128)             0         ['dropout_3[0][0]',           \n","                                                                     'dot_2[0][0]']               \n","                                                                                                  \n"," lambda_3 (Lambda)           (None, 119, 128)             0         ['subtract_1[0][0]']          \n","                                                                                                  \n"," concatenate (Concatenate)   (None, 119, 512)             0         ['dropout_2[0][0]',           \n","                                                                     'dot_1[0][0]',               \n","                                                                     'multiply[0][0]',            \n","                                                                     'lambda_2[0][0]']            \n","                                                                                                  \n"," concatenate_1 (Concatenate  (None, 119, 512)             0         ['dropout_3[0][0]',           \n"," )                                                                   'dot_2[0][0]',               \n","                                                                     'multiply_1[0][0]',          \n","                                                                     'lambda_3[0][0]']            \n","                                                                                                  \n"," bidirectional_1 (Bidirecti  (None, 119, 256)             656384    ['concatenate[0][0]',         \n"," onal)                                                               'concatenate_1[0][0]']       \n","                                                                                                  \n"," dropout_4 (Dropout)         (None, 119, 256)             0         ['bidirectional_1[0][0]']     \n","                                                                                                  \n"," dropout_5 (Dropout)         (None, 119, 256)             0         ['bidirectional_1[1][0]']     \n","                                                                                                  \n"," lambda_4 (Lambda)           (None, 256)                  0         ['dropout_4[0][0]']           \n","                                                                                                  \n"," lambda_6 (Lambda)           (None, 256)                  0         ['dropout_4[0][0]']           \n","                                                                                                  \n"," lambda_5 (Lambda)           (None, 256)                  0         ['dropout_5[0][0]']           \n","                                                                                                  \n"," lambda_7 (Lambda)           (None, 256)                  0         ['dropout_5[0][0]']           \n","                                                                                                  \n"," concatenate_2 (Concatenate  (None, 1024)                 0         ['lambda_4[0][0]',            \n"," )                                                                   'lambda_6[0][0]',            \n","                                                                     'lambda_5[0][0]',            \n","                                                                     'lambda_7[0][0]']            \n","                                                                                                  \n"," dense (Dense)               (None, 32)                   32800     ['concatenate_2[0][0]']       \n","                                                                                                  \n"," dense_1 (Dense)             (None, 1)                    33        ['dense[0][0]']               \n","                                                                                                  \n","==================================================================================================\n","Total params: 8517297 (32.49 MB)\n","Trainable params: 8517297 (32.49 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["from tensorflow.keras.layers import Input, Embedding, Bidirectional, Flatten, Dot, Dropout, LSTM, Activation, TimeDistributed, Dense, Subtract, Lambda, Multiply, Concatenate, GlobalMaxPooling1D\n","from tensorflow.keras.models import Model\n","import tensorflow as tf\n","from tensorflow.keras.layers import Layer\n","import tensorflow.python.keras.backend as K\n","from tensorflow.keras.regularizers import l2\n","\n","# del model_NLI\n","\n","embedding_DROPOUT = 0.2\n","DROPOUT = 0.2\n","L2 = 0.001\n","lstm_size = 64\n","\n","K.clear_session()\n","\n","# input layer\n","input1 = Input(shape=(maxlen1, ))\n","input2 = Input(shape=(maxlen2, ))\n","\n","# embedding layer\n","embedding = Embedding(input_dim=vocab_size, output_dim = vector_size, embeddings_regularizer=l2(L2))\n","\n","lstm_output1 = embedding(input1)\n","lstm_output1 = Dropout(embedding_DROPOUT)(lstm_output1)\n","lstm_output2 = embedding(input2)\n","lstm_output2 = Dropout(embedding_DROPOUT)(lstm_output2)\n","\n","# BiLSTM\n","lstm = Bidirectional(LSTM(lstm_size, return_sequences=True, kernel_regularizer=l2(L2)))\n","\n","lstm_output1 = lstm(lstm_output1)\n","lstm_output1 = Dropout(DROPOUT)(lstm_output1)\n","lstm_output2 = lstm(lstm_output2)\n","lstm_output2 = Dropout(DROPOUT)(lstm_output2)\n","\n","# attention weights\n","attention = Dot(axes=-1)([lstm_output1, lstm_output2])\n","weight_att_1 = Lambda(lambda x: tf.keras.activations.softmax(x, axis=1))(attention)\n","weight_att_2 = Lambda(lambda x: tf.keras.activations.softmax(x, axis=2))(attention)\n","aligned_1 = Dot(axes=(1, 1))([weight_att_1, lstm_output2])\n","aligned_2 = Dot(axes=(2, 1))([weight_att_2, lstm_output1])\n","\n","# features concatenation\n","feature_1 = Concatenate()([lstm_output1, aligned_1, Multiply()([lstm_output1, aligned_1]), Lambda(lambda x: tf.abs(x))(Subtract()([lstm_output1, aligned_1]))])\n","feature_2 = Concatenate()([lstm_output2, aligned_2, Multiply()([lstm_output2, aligned_2]), Lambda(lambda x: tf.abs(x))(Subtract()([lstm_output2, aligned_2]))])\n","\n","# BiLSTM\n","lstm_2 = Bidirectional(LSTM(lstm_size*2, return_sequences=True, kernel_regularizer=l2(L2)))\n","\n","lstm2_output1 = lstm_2(feature_1)\n","lstm2_output1 = Dropout(DROPOUT)(lstm2_output1)\n","lstm2_output2 = lstm_2(feature_2)\n","lstm2_output2 = Dropout(DROPOUT)(lstm2_output2)\n","\n","# pooling\n","premise_avg = Lambda(lambda x: tf.reduce_mean(x, axis=1))(lstm2_output1)\n","hypothesis_avg = Lambda(lambda x: tf.reduce_mean(x, axis=1))(lstm2_output2)\n","premise_max = Lambda(lambda x: tf.reduce_max(x, axis=1))(lstm2_output1)\n","hypothesis_max = Lambda(lambda x: tf.reduce_max(x, axis=1))(lstm2_output2)\n","\n","final_feature = Concatenate()([premise_avg, premise_max, hypothesis_avg, hypothesis_max])\n","\n","# dense\n","dense1 = Dense(32, activation='tanh', kernel_regularizer=l2(L2))(final_feature)\n","dense2 = Dense(1, activation='sigmoid', kernel_regularizer=l2(L2))(dense1)\n","\n","# output\n","model_NLI = Model(inputs=[input1, input2], outputs=dense2)\n","\n","# compile\n","model_NLI.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# whether use the word vectors of word2vec\n","# embedding.set_weights([embedding_matrix])\n","# embedding.set_weights([embedding_matrix])\n","# print(\"use weights from word2vec\")\n","\n","model_NLI.summary()\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":161251,"status":"ok","timestamp":1713968183664,"user":{"displayName":"Kang Fan","userId":"02418101774769161322"},"user_tz":-60},"id":"iKLouCQrFcu7","outputId":"ed891303-7011-4623-fda6-0cfc74c73e00"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","337/337 [==============================] - 44s 82ms/step - loss: 1.1215 - accuracy: 0.5685 - val_loss: 0.7136 - val_accuracy: 0.6271\n","Epoch 2/10\n","337/337 [==============================] - 23s 68ms/step - loss: 0.7203 - accuracy: 0.6318 - val_loss: 0.7191 - val_accuracy: 0.6318\n","Epoch 3/10\n","337/337 [==============================] - 25s 75ms/step - loss: 0.7105 - accuracy: 0.6552 - val_loss: 0.7006 - val_accuracy: 0.6437\n","Epoch 4/10\n","337/337 [==============================] - 23s 69ms/step - loss: 0.6985 - accuracy: 0.6821 - val_loss: 0.7081 - val_accuracy: 0.6446\n","Epoch 5/10\n","337/337 [==============================] - 23s 69ms/step - loss: 0.6809 - accuracy: 0.7015 - val_loss: 0.7313 - val_accuracy: 0.6320\n","Epoch 6/10\n","337/337 [==============================] - ETA: 0s - loss: 0.6691 - accuracy: 0.7177Restoring model weights from the end of the best epoch: 3.\n","337/337 [==============================] - 23s 67ms/step - loss: 0.6691 - accuracy: 0.7177 - val_loss: 0.7299 - val_accuracy: 0.6364\n","Epoch 6: early stopping\n"]}],"source":["from keras.callbacks import EarlyStopping\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n","\n","# train the model\n","history = model_NLI.fit([padded_p_train, padded_h_train], padded_label_train, batch_size=64, epochs=10, validation_data = ([padded_p_val, padded_h_val], padded_label_val), callbacks=[early_stopping])"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52603,"status":"ok","timestamp":1713968236257,"user":{"displayName":"Kang Fan","userId":"02418101774769161322"},"user_tz":-60},"id":"BUuCznkGC5Cy","outputId":"255aaa55-61a0-49bb-fee7-31e2efead086"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["model_NLI.save(\"model.h5\")"]},{"cell_type":"markdown","metadata":{"id":"F80jbs8xaa_N"},"source":["### The second section is the demo part which produces the prediction result"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1300,"status":"ok","timestamp":1713968237546,"user":{"displayName":"Kang Fan","userId":"02418101774769161322"},"user_tz":-60},"id":"wF5SQhefFcu7","outputId":"84c26c70-c92a-4fc3-a826-7a2745594229"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import pandas as pd\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","df_2 = pd.read_csv('test.csv', na_values=\"n/a\")\n","premise_data_dev = df_2['premise'].tolist()\n","hypothesis_data_dev = df_2['hypothesis'].tolist()\n","# label_data_dev = df_2['label'].tolist()\n","\n","# data clean\n","paired_data_dev = list(zip(premise_data_dev, hypothesis_data_dev))\n","\n","duplicates = set()\n","unique_paired_data_dev = []\n","for pair in paired_data_dev:\n","#     print(pair)\n","    if (pair in duplicates) or (pair[0] == pair[1]) or pd.isna(pair[0]) or pd.isna(pair[1]):\n","        continue\n","    else:\n","        duplicates.add(pair)\n","        unique_paired_data_dev.append(pair)\n","\n","premise_data_dev, hypothesis_data_dev = zip(*unique_paired_data_dev)\n","\n","premise_data_clean_garbled_dev = [re.sub(r'[^a-zA-Z0-9\\s]', '', text) for text in premise_data_dev]\n","hypothesis_data_clean_garbled_dev = [re.sub(r'[^a-zA-Z0-9\\s]', '', text) for text in hypothesis_data_dev]\n","\n","cleaned_premise_data_dev = [' '.join(re.sub(r'\\b\\w*www\\w*\\b', '', text).split()) for text in premise_data_clean_garbled_dev]\n","cleaned_hypothesis_data_dev = [' '.join(re.sub(r'\\b\\w*www\\w*\\b', '', text).split()) for text in hypothesis_data_clean_garbled_dev]\n","\n","preprocessed_premise_data_dev = [word_tokenize(text) for text in cleaned_premise_data_dev]\n","filtered_premise_data_dev = [[word.lower() for word in premise if word.lower() not in stop_words] for premise in preprocessed_premise_data_dev]\n","\n","preprocessed_hypothesis_data_dev = [word_tokenize(text) for text in cleaned_hypothesis_data_dev]\n","filtered_hypothesis_data_dev = [[word.lower() for word in hypothesis if word.lower() not in stop_words] for hypothesis in preprocessed_hypothesis_data_dev]\n","\n","lemmatized_premise_data_dev = [[lemmatizer.lemmatize(word) for word in premise] for premise in filtered_premise_data_dev]\n","lemmatized_hypothesis_dat_dev = [[lemmatizer.lemmatize(word) for word in hypothesis] for hypothesis in filtered_hypothesis_data_dev]\n","\n","premise_vectors_dev = []\n","for sentence in filtered_premise_data_dev:\n","    sentence_vectors_dev = []\n","    for word in sentence:\n","        try:\n","            sentence_vectors_dev.append(word_vectors.key_to_index[word])\n","        except KeyError:\n","            sentence_vectors_dev.append(vector_size)\n","    premise_vectors_dev.append(sentence_vectors_dev)\n","\n","hypothesis_vectors_dev = []\n","for sentence in filtered_hypothesis_data_dev:\n","    sentence_vectors_dev = []\n","    for word in sentence:\n","        try:\n","            sentence_vectors_dev.append(word_vectors.key_to_index[word])\n","        except KeyError:\n","            sentence_vectors_dev.append(vector_size)\n","    hypothesis_vectors_dev.append(sentence_vectors_dev)\n","\n","irregular_array = np.array(premise_vectors_dev, dtype=object)\n","padded_p_test = pad_sequences(irregular_array, padding='post', dtype='float32', maxlen=maxlen1)\n","irregular_array = np.array(hypothesis_vectors_dev, dtype=object)\n","padded_h_test = pad_sequences(irregular_array, padding='post', dtype='float32', maxlen=maxlen2)\n","# padded_label_test = np.array(label_data_dev)\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7107,"status":"ok","timestamp":1713968244650,"user":{"displayName":"Kang Fan","userId":"02418101774769161322"},"user_tz":-60},"id":"gqpGgUbK8k3R","outputId":"2b6c09cb-4d69-4fdb-f358-bb0f4b407f61"},"outputs":[{"name":"stdout","output_type":"stream","text":["104/104 [==============================] - 4s 17ms/step\n"]}],"source":["from tensorflow.keras.models import load_model\n","\n","model_NLI = load_model('model.h5')\n","predictions_test = model_NLI.predict([padded_p_test, padded_h_test])\n","binary_predictions = np.where(predictions_test >= 0.5, 1, 0)\n","df = pd.DataFrame(binary_predictions, columns=['prediction'])\n","\n","df.to_csv('Group_36_B.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"zmczS7PGannu"},"source":["### The next part is developemtn part for evaluation according to accuracy"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7379,"status":"ok","timestamp":1713968252018,"user":{"displayName":"Kang Fan","userId":"02418101774769161322"},"user_tz":-60},"id":"esAc7mG5zvsB","outputId":"9a481a89-7271-4925-9414-afb0a6f7e1f9"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["211/211 [==============================] - 4s 17ms/step\n","Accuracy:  0.64651715431457\n","f1:  0.6461957659088589\n"]}],"source":["from sklearn.metrics import f1_score\n","import pandas as pd\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","df_2 = pd.read_csv('dev.csv', na_values=\"n/a\")\n","premise_data_dev = df_2['premise'].tolist()\n","hypothesis_data_dev = df_2['hypothesis'].tolist()\n","label_data_dev = df_2['label'].tolist()\n","\n","# data clean\n","paired_data_dev = list(zip(premise_data_dev, hypothesis_data_dev, label_data_dev))\n","\n","duplicates = set()\n","unique_paired_data_dev = []\n","for pair in paired_data_dev:\n","#     print(pair)\n","    if (pair in duplicates) or (pair[0] == pair[1]) or pd.isna(pair[0]) or pd.isna(pair[1]):\n","        continue\n","    else:\n","        duplicates.add(pair)\n","        unique_paired_data_dev.append(pair)\n","\n","premise_data_dev, hypothesis_data_dev, label_data_dev = zip(*unique_paired_data_dev)\n","\n","premise_data_clean_garbled_dev = [re.sub(r'[^a-zA-Z0-9\\s]', '', text) for text in premise_data_dev]\n","hypothesis_data_clean_garbled_dev = [re.sub(r'[^a-zA-Z0-9\\s]', '', text) for text in hypothesis_data_dev]\n","\n","cleaned_premise_data_dev = [' '.join(re.sub(r'\\b\\w*www\\w*\\b', '', text).split()) for text in premise_data_clean_garbled_dev]\n","cleaned_hypothesis_data_dev = [' '.join(re.sub(r'\\b\\w*www\\w*\\b', '', text).split()) for text in hypothesis_data_clean_garbled_dev]\n","\n","preprocessed_premise_data_dev = [word_tokenize(text) for text in cleaned_premise_data_dev]\n","filtered_premise_data_dev = [[word.lower() for word in premise if word.lower() not in stop_words] for premise in preprocessed_premise_data_dev]\n","\n","preprocessed_hypothesis_data_dev = [word_tokenize(text) for text in cleaned_hypothesis_data_dev]\n","filtered_hypothesis_data_dev = [[word.lower() for word in hypothesis if word.lower() not in stop_words] for hypothesis in preprocessed_hypothesis_data_dev]\n","\n","lemmatized_premise_data_dev = [[lemmatizer.lemmatize(word) for word in premise] for premise in filtered_premise_data_dev]\n","lemmatized_hypothesis_dat_dev = [[lemmatizer.lemmatize(word) for word in hypothesis] for hypothesis in filtered_hypothesis_data_dev]\n","\n","premise_vectors_dev = []\n","for sentence in filtered_premise_data_dev:\n","    sentence_vectors_dev = []\n","    for word in sentence:\n","        try:\n","            sentence_vectors_dev.append(word_vectors.key_to_index[word])\n","        except KeyError:\n","            sentence_vectors_dev.append(vector_size)\n","    premise_vectors_dev.append(sentence_vectors_dev)\n","\n","hypothesis_vectors_dev = []\n","for sentence in filtered_hypothesis_data_dev:\n","    sentence_vectors_dev = []\n","    for word in sentence:\n","        try:\n","            sentence_vectors_dev.append(word_vectors.key_to_index[word])\n","        except KeyError:\n","            sentence_vectors_dev.append(vector_size)\n","    hypothesis_vectors_dev.append(sentence_vectors_dev)\n","\n","irregular_array = np.array(premise_vectors_dev, dtype=object)\n","padded_p_test = pad_sequences(irregular_array, padding='post', dtype='float32', maxlen=maxlen1)\n","irregular_array = np.array(hypothesis_vectors_dev, dtype=object)\n","padded_h_test = pad_sequences(irregular_array, padding='post', dtype='float32', maxlen=maxlen2)\n","padded_label_test = np.array(label_data_dev)\n","\n","\n","predictions = model_NLI.predict([padded_p_test, padded_h_test])\n","\n","threshold = 0.5\n","binary_predictions = np.where(predictions >= threshold, 1, 0)\n","\n","\n","f1 = f1_score(label_data_dev, binary_predictions, average='weighted')\n","accuracy = np.mean(binary_predictions.squeeze() == label_data_dev)\n","print(\"Accuracy: \", accuracy)\n","print(\"f1: \", f1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PxBqcakff-0K"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
